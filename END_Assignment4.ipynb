{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END_Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjml8ZasXgKPx7bK678hWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnagorrepati/NLP/blob/main/END_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC7FDLgpCGY5"
      },
      "source": [
        "\n",
        "**2 - Updated Sentiment Analysis**\n",
        "\n",
        "In the previous notebook, we got the fundamentals down for sentiment analysis. In this notebook, we'll actually get decent results.\n",
        "\n",
        "We will use:\n",
        "\n",
        "* packed padded sequences\n",
        "* pre-trained word embeddings\n",
        "* different RNN architecture\n",
        "* bidirectional RNN\n",
        "* multi-layer RNN\n",
        "* regularization\n",
        "* a different optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMlLe5HH78ox"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzrnSayzCyV7"
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW67owBjqW7E"
      },
      "source": [
        "for i in train_data:\n",
        "  i.text=i.text[::-1]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX3AbmvKqcWx",
        "outputId": "43c61013-91fb-4ca4-fec0-a256cc4a5329"
      },
      "source": [
        "for i in train_data[0:5]:\n",
        "  print(i.text)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.', 'age', 'any', 'of', 'person', 'a', 'for', 'meaningful', 'be', 'can', 'which', 'film', 'family', 'powerful', 'a', 'is', 'This', '.', 'lives', 'their', 'of', 'part', 'major', 'a', 'obviously', 'is', 'and', 'terms', 'basic', 'in', 'shown', 'is', 'family', 'the', 'of', 'faith', 'The', '.', 'sugarcoating', 'or', 'sentiment', 'undo', 'without', 'depicted', 'is', 'one', 'loved', 'a', 'of', 'illness', 'devastating', 'a', 'with', 'deal', 'family', 'his', 'and', 'Dan', 'How', '.', 'team', 'real', 'a', 'make', 'to', 'takes', 'it', 'what', 'shows', ',', 'teammates', 'his', 'and', 'coach', \"'s\", 'Dan', 'of', 'determination', 'and', 'commitment', 'The', '.', 'us', 'of', 'all', 'for', 'lesson', 'life', 'a', 'is', 'races', 'losing', 'and', 'winning', 'handle', 'family', 'his', 'of', 'rest', 'the', 'and', 'Dan', 'How', '.', 'throughout', 'evident', 'is', 'members', 'family', 'the', 'of', 'support', 'and', 'love', 'The', '.', 'adulthood', 'full', 'through', 'age', 'young', 'a', 'from', 'life', \"'s\", 'athlete', 'this', 'depicts', 'movie', 'The', '.', 'skater', 'speed', 'Olympic', 'champion', 'a', ',', 'Jansen', 'Dan', 'of', 'biography', 'a', 'is', 'This', '.', 'film', 'family', 'wonderful', 'a', 'is', 'Promise', \"'s\", 'Brother', 'A']\n",
            "['.', 'performances', 'their', 'with', 'it', 'bring', 'to', 'fail', 'never', 'that', 'actresses', 'character', 'underrated', 'Two', '.', 'again', ')', 'Horrors', 'of', 'Shop', 'Little', '(', 'Greene', 'Ellen', 'and', ')', 'Garp', 'to', 'According', 'World', '(', 'Kurtz', 'Swoosie', 'seeing', '/>Loved', '/><br', 'br', '<', '.hmmmm', '\\x85', 'creation', 'Fuller', 'Brian', 'another', 'is', 'which', ',', 'Wonderfalls', 'rent', 'go', 'MUST', 'you', 'Daisies', 'Pushing', 'enjoy', 'you', 'If', '.', '2004', 'from', 'series', 'lived', 'short', ',', 'Great', '!', '!', '!', 'Wonderfalls', '.', 'up', 'it', 'looked', 'just', 'and', 'from', 'him', 'knew', 'I', 'where', 'remember', 'to', 'trying', 'kept', 'I', '.', 'too', 'job', 'great', 'a', 'does', 'Pace', '/>Lee', '/><br', 'Deschanel\".<br', 'Zooey', 'get', 'they', \"n't\", 'did', 'Why', '\"', 'thinking', 'keep', 'I', 'that', 'distraction', 'a', 'of', 'more', \"'s\", 'It', '.', 'her', 'to', 'attached', 'get', 'easily', 'you', 'and', 'lovable', 'and', 'sweet', 'very', 'is', 'character', 'Her', '.', 'job', 'great', 'a', 'does', 'Friel', 'Anna', ',', 'wrong', 'me', 'get', \"n't\", 'Do', '.', 'Zooey', 'see', 'I', 'screen', 'the', 'on', 'her', 'see', 'I', 'time', 'Every', '.', 'Deschanel', 'Zooey', \"'s\", 'man', 'poor', 'a', 'playing', 'is', 'Friel', 'Anna', 'yes', ',', 'forums', 'the', 'in', 'seen', \"'ve\", 'I', '/>As', '/><br', 'is.<br', 'it', 'where', 'know', 'to', 'enough', 'researched', \"n't\", 'have', 'just', 'I', ',', 'influence', 'STRONG', 'a', 'or', ',', 'connection', 'a', 'be', 'to', 'has', 'There', '.', 'Burton', 'Tim', 'scream', 'all', 'sets', 'external', 'the', ',', 'music', 'the', ',', 'narration', 'the', ',', 'cinematography', 'The', '.', 'Factory', 'Chocolate', 'the', '&', 'Charlie', ',', 'Fish', 'Big', ',', 'Sissorhands', 'Edward', '...', 'films', 'better', \"'s\", 'Burton', 'Tim', 'of', 'reeks', 'show', 'The', '.', 'pilot', 'sweet', 'Very']\n",
            "['.', 'enjoys', 'COUNTRY', 'HIGH', 'THE', 'RIDE', '7-something', 'the', 'to', 'compared', ',', 'rating', 'higher', 'a', 'deserve', 'would', '/>Definitely', '/><br', 'best.<br', 'his', 'not', 'if', ',', 'best', 'his', 'of', 'one', 'as', 'shines', 'still', 'HUNT', 'LAST', 'THE', ',', 'one', 'this', 'as', 'career', 'prolific', 'as', 'an', 'in', 'even', 'but', ',', 'genre', 'imaginable', 'every', 'nearly', 'in', 'successful', 'was', 'and', 'movies', 'great', 'really', 'of', 'couple', 'a', 'quite', 'made', 'has', 'craftsmanship!He', 'Master', '.', 'storyteller', 'superb', 'a', 'really', 'is', 'BROOKS', 'director', '/>And', '/><br', 'well.<br', 'very', 'picture', 'the', 'fits', 'soundtrack', 'the', 'even', 'that', 'and', 'chosen', 'cleverly', 'are', 'locations', 'the', 'all', 'that', ',', ')', 'photographed', 'superbly', 'all', 'also', 'are', 'which', ',', 'Mann', 'Anthony', 'by', 'directed', 'ones', 'the', 'from', 'aside', '(', 'shot', 'well', 'that', 'western', 'a', 'seen', 'seldomely', 'have', 'I', ',', 'photograped', 'superbly', 'is', 'HUNT', 'LAST', 'THE', 'that', ',', 'mentioned', 'be', 'also', 'should', 'it', ',', 'dialogue', 'great', 'the', 'and', 'script', 'intelligent', 'the', ',', 'involved', 'everybody', 'of', 'performances', 'top', 'the', 'from', '/>Aside', '/><br', 'br', '<', '.', 'him', 'got', 'buffalo', 'the', 'end', 'the', 'in', 'but', ',', 'buffalo', 'his', 'got', 'has', 'Taylor', 'Mr.', '.', 'again', 'present', 'are', ',', 'began', 'hatred', 'the', 'all', 'when', ',', 'past', 'the', 'from', 'echos', 'tree)and', 'a', 'on', '(', 'sight', 'into', 'comes', 'skin', 'buffalo', 'white', 'a', 'Taylor', 'Mr.', 'from', 'away', 'and', 'on', 'moves', 'camera', 'the', 'When', '.', 'gem', 'this', 'of', 'frames', 'last', 'the', 'and', 'ending', 'its', 'forget', 'to', 'able', 'be', 'will', 'movie', 'this', 'seen', 'ever', 'has', 'who', '/>Nobody', '/><br', 'them.<br', 'of', 'one', 'for', 'out', 'reaches', 'fate', 'until', 'killers', '-', 'buffalo', 'of', 'life', 'the', 'leading', ',', 'Inian', 'young', 'a', 'and', 'man', 'old', 'crippled', 'a', ',', 'Padget', 'Ms.', 'beautiful', 'by', 'surrounded', ',', 'choices', 'other', 'have', 'to', 'seeming', 'not', 'by', 'partly', ',', 'hate', 'by', 'partly', ',', 'together', 'bound', 'men', 'Two', '.', 'one', 'this', 'as', 'depressive', 'and', 'bleak', 'as', 'western', 'any', 'remember', 'not', 'can', 'I', ')', 'genre', 'this', 'like', 'I', '(', 'western', 'of', '100s', 'seen', 'certainly', 'having', 'and', 'hard', 'thinking', '/>Although', '/><br', 'br', '<', '...', 'is', 'still', 'it', 'maybe', 'or', ',', 'time', 'its', 'of', 'ahead', 'decades', '3', 'probably', 'was', 'it', 'Actually', '.', 'earlier', 'years', 'dozen', 'a', 'half', 'even', 'was', 'and', 'movie', 'better', 'the', 'definitely', 'is', 'HUNT', 'LAST', 'known', 'hardly', 'the', 'but', ',', 'west', 'old', 'the', 'in', 'life', 'at', 'look', 'realistic', 'a', 'had', 'which', ',', 'western', 'later', 'best', 'and', 'first', 'the', 'of', 'one', 'as', 'COUNTRY', 'HIGH', 'THE', 'RIDE', \"'s\", 'Peckinpah', 'regard', 'Manr', '!', 'movie', 'seen', 'be', 'to', 'worth', 'well', 'and', 'great', 'a', 'still', 'It', '.', 'time', 'with', 'impact', 'its', 'of', 'anything', 'lost', 'not', 'has', 'it', 'and', ')', 'watch', 'another', 'it', 'gave', 'I', 'ago', 'days', 'few', 'a', 'just', '(', 'then', 'since', 'times', 'more', 'three', 'or', 'two', 'it', 'seen', 'have', 'I', '.', 'time', 'first', 'it', 'saw', 'I', 'since', 'decades', '2', 'than', 'more', 'the', 'in', 'forever', 'me', 'with', 'stayed', 'has', 'movie', 'this', 'but', ',', 'right', 'only', 'not', 'was', 'he', 'say', 'must', 'I', 'and', ',', 'then', 'me', 'to', 'it', 'recommended', 'and', 'highly', 'film', 'this', 'regarded', 'always', 'has', 'Dad', 'My', '.', 'teenager', 'a', 'was', 'I', 'when', ',', ')', 'screen', 'big', 'the', 'on', 'never', ',', 'TV', 'on', '(', 'movie', 'this', 'seen', 'first', 'have', 'I', 'reviewers', 'other', 'most', 'Like']\n",
            "['>', '/', '/><br', 'again.<br', 'it', 'see', 'to', 'want', 'me', 'made', ',', 'fresh', ',', 'lovely', ',', 'brilliant', 'was', 'It', '.', 'scenes', 'sex', ',', 'nude', 'unnecessary', 'any', 'us', 'show', \"n't\", 'did', 'it', 'And', '.', 'comedies', 'romantic', 'typical', 'in', 'seen', 'be', 'can', 'that', 'cliches', 'any', 'avoid', 'to', 'hard', 'tried', 'movie', '/>This', '/><br', 'usual.<br', 'as', 'charming', 'also', 'was', 'nul', 'ha', 'Kim', 'Actress', '!', '!', 'revelation', 'a', 'such', 'was', 'He', '.', 'excellency', 'with', 'job', 'his', 'did', 'woo', 'sang', 'kwan', 'newcomer', 'that', 'surprised', 'pleasantly', 'was', '/>I', '/><br', 'friends.<br', 'great', 'being', 'up', 'end', 'two', 'these', ',', 'quarrells', 'hilarious', 'some', 'Through', '.', 'age', \"'s\", 'character', 'female', 'actually', 'is', 'He', '.', 'guy', 'student', 'school', 'high', 'charistmatic', 'but', 'rich', ',', 'spoiled', 'the', 'teach', 'to', 'forced', 'student', 'university', 'is', 'who', 'character', 'female', '/>Leading', '/><br', 'back.<br', 'faith', 'my', 'brought', 'one', 'this', 'But', '.', 'business', 'movie', 'Korean', 'in', 'faith', 'my', 'lost', 'nearly', \"'d\", 'I', ',', '\"', 'Mafia', 'Marrying', '\"', 'seen', 'having', 'After']\n",
            "['.', 'fun', 'is', 'it', 'but', ',', 'people', 'seven', 'among', 'place', 'take', 'would', 'events', 'these', 'all', 'that', 'unlikely', 'bit', 'a', 'looks', 'it', ',', 'capital', 'Czech', 'the', 'of', 'size', 'actual', 'the', 'Given', '.', 'magnitude', 'of', 'orders', 'four', 'by', 'smaller', 'were', 'Prague', 'if', 'natural', 'be', 'would', 'that', 'ways', 'exciting', 'and', 'interesting', 'in', 'interact', 'characters', 'seven', '/>The', '/><br', 'dinner.<br', \"'\", 'parents', 'Hanka', 'during', 'movies', 'their', 'shooting', 'are', 'who', 'tourists', 'Japanese', 'so', 'or', '20', 'for', 'scenes', 'hysterical', 'some', 'translate', 'to', 'has', 'she', 'eventually', 'and', '-', 'translator', 'a', 'as', '-', 'agency', 'travel', 'the', 'for', 'works', 'also', 'Lenka', ')', '.', 'life', 'his', 'saves', 'Ond&#345;ej', 'and', 'hospital', 'the', 'to', 'victim', 'the', 'bring', 'Hanka', 'and', 'Jakub', '-', 'accident', 'car', 'important', 'an', 'of', 'victim', 'a', 'of', 'brother', 'a', 'is', 'he', 'because', 'debt', 'his', 'pays', 'magician', 'The', '(', '.', 'again', 'Hanka', 'capture', 'to', 'try', 'can', 'Ond&#345;ej', 'that', 'so', 'disappear', 'him', 'make', 'to', 'magician', 'a', 'asks', 'Ond&#345;ej', 'after', 'even', '-', 'marriage', 'their', 'stabilize', 'and', 'him', 'forgive', 'to', 'ready', 'always', 'is', 'Lenka', 'wife', \"'s\", 'Ond&#345;ej', ',', '/>Finally', '/><br', 'happy.<br', 'very', 'him', 'make', 'that', 'news', 'the', 'about', 'learns', 'Ond&#345;ej', 'how', 'is', 'which', '-', 'Hanka', 'with', 'up', 'broke', 'he', 'that', 'audience', 'his', 'to', 'announces', 'He', '.', 'loses', 'eventually', 'he', 'that', 'job', 'a', '-', 'job', 'his', 'likes', 'who', 'one', 'only', 'the', 'is', 'he', 'and', 'station', 'radio', 'the', 'in', 'works', '/>Petr', '/><br', 'problems.<br', 'their', 'from', 'characters', 'other', 'the', 'helps', 'sometimes', 'but', 'confused', 'pretty', 'seems', 'She', '.', 'UFOs', 'or', 'dad', 'her', 'see', 'to', 'order', 'in', 'Czechia', 'to', 'came', 'she', 'whether', 'learn', \"n't\", 'wo', 'you', 'and', '-', 'barmaid', 'a', 'as', 'works', 'Macedonia', 'from', 'Prague', 'to', 'came', 'who', ')', '\"', 'Spring', '\"', 'for', 'word', 'Slavic', 'a', '(', '/>Vesna', '/><br', 'depression.<br', 'the', 'with', 'deal', 'to', 'ways', 'own', 'his', 'has', 'he', 'and', 'hospital', 'a', 'in', 'dies', 'mother', 'his', ',', 'eventually', ';', 'women', 'different', 'many', 'with', 'sleeps', 'usually', 'he', 'and', 'anything', 'about', 'serious', 'never', 'is', '-', 'marijuana', 'with', 'Jakub', 'provides', 'also', 'who', '-', 'Robert', '.', 'tourists', 'Japanese', 'to', 'people', 'Czech', 'ordinary', 'of', 'life', 'the', 'show', 'to', 'is', 'job', 'his', 'where', 'agency', 'travel', 'a', 'for', 'works', 'also', 'who', 'matchmaker', 'a', ',', 'Robert', 'by', 'organized', 'is', 'it', ',', 'up', 'break', 'Petr', 'and', 'Hanka', '/>When', '/><br', 'parents.<br', 'her', 'to', 'returns', 'and', 'disappointed', 'is', 'Hanka', '.', 'girlfriend', 'another', 'has', 'already', 'he', 'that', 'Jakub', 'inform', 'band', 'his', 'from', 'friends', 'the', ',', 'However', '.', 'drugs', 'the', 'by', 'devastated', 'rather', 'be', 'to', 'seems', 'memory', 'whose', 'addict', 'drug', 'innocent', 'an', ',', 'Jakub', 'with', 'plans', 'serious', 'have', 'to', 'seems', 'Hanka', ',', 'while', 'a', '/>For', '/><br', 'go.<br', 'boyfriends', 'her', 'of', 'type', 'the', 'as', 'far', 'as', ',', 'figure', 'flexible', 'very', 'a', 'is', 'she', 'But', '.', 'environment', 'better', 'a', 'creating', 'in', 'unsuccessful', 'rather', 'seems', 'but', 'example', 'good', 'a', 'as', 'life', 'bourgeois', \"'\", 'parents', 'her', 'view', 'not', 'does', 'Hanka', '.', 'station', 'radio', 'private', 'a', 'in', 'works', 'who', 'Petr', 'with', 'up', 'break', 'to', '-', 'coin', 'a', 'up', 'tossing', 'by', '-', 'decides', 'just', 'She', '.', 'parents', 'her', 'with', 'relationship', 'mixed', 'very', 'a', 'has', 'Hanka', ',', '/>Meanwhile', '/><br', 'burns.<br', 'repeatedly', 'he', 'that', 'house', 'a', '-', 'house', \"'\", 'parents', \"'s\", 'Hanka', 'into', 'get', 'to', 'plumber', 'a', 'as', 'up', 'dresses', 'repeatedly', 'he', 'that', 'obsessed', 'so', 'is', 'He', '.', 'Hanka', ',', 'woman', 'another', 'loved', 'he', 'much', 'how', 'prove', 'to', 'neurobiology', 'studied', 'only', 'has', 'he', 'that', 'learn', 'you', ',', 'Nevertheless', '.', 'daughters', 'two', 'has', 'who', 'surgeon', 'young', 'married', 'and', 'talented', 'a', 'is', '/>Ond&#345;ej', '/><br', 'analysis.<br', 'psychological', \"'s\", 'movie', 'the', 'of', 'feature', 'realistic', 'very', 'a', \"'s\", 'that', ';', 'context', 'the', 'on', 'depending', 'others', 'of', 'features', 'the', 'judge', 'characters', 'the', 'how', 'see', 'can', 'you', ',', 'Also', '.', 'either', 'character', 'positive', 'permanently', 'a', 'is', 'them', 'of', 'neither', 'and', 'one', 'negative', 'universally', 'a', 'be', 'to', 'designed', 'is', 'characters', 'the', 'of', 'neither', 'that', 'is', 'movies', 'Czech', 'the', 'of', 'feature', 'typical', 'Another', '.', 'recombining', 'and', ',', 'combining', ',', 'up', 'breaking', 'are', 'relationships', 'Their', '.', '1990s', 'early', 'the', 'in', 'movies', 'U.S.', 'the', 'among', 'popular', 'was', 'theme', 'this', 'that', 'indicates', 'web', 'the', ';', 'life', 'dirty', ',', 'difficult', 'pretty', 'a', 'have', 'to', 'seem', 'characters', 'central', 'seven', 'the', ',', 'movies', 'Czech', 'other', 'many', 'in', 'like', '/>Much', '/><br', 'good.<br', 'pretty', 'was', 'it', 'and', ')', '2000', ',', 'Loners', '(', 'Samotá&#345;i', 'saw', 'I', 'time', 'first', 'the', 'was', 'It', '.', ')', 'VES', '(', 'Studies', 'Environmental', 'and', 'Visual', 'of', 'Department', 'the', 'in', ')', 'subtitles', 'English', 'with', '(', 'movie', 'Czech', 'a', 'of', 'screening', 'to', 'us', 'invited', ',', 'Czech', 'speaks', 'who', 'girl', 'Chinese', 'a', ',', 'Ying']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ44CglHC5fr"
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeCd0sf7C-iB"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvGRmTAyDDE1"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D4QuU7wQyOX",
        "outputId": "9fbd9e24-3d50-47da-d376-6b49757b1dd6"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Q1IDoiDqbs"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjekQhuGD7ei"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 3\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtdt_PjAD8qB",
        "outputId": "b5919720-d941-4626-a443-c98fae30abd7"
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,387,817 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwd8DousEBY_",
        "outputId": "d8e22629-d722-476c-ebd0-c39a835b2484"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fClyoWWgEGKM",
        "outputId": "5cac0305-4a54-4246-e964-85960895907f"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.3970,  0.4024,  1.0612,  ..., -0.0136, -0.3363,  0.6442],\n",
              "        [-0.5197,  1.0395,  0.2092,  ..., -0.8857, -0.2294,  0.1244],\n",
              "        [ 0.0057, -0.0707, -0.0804,  ..., -0.3292, -0.0130,  0.0716]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvCrrcCmEJJ-",
        "outputId": "f811885b-b21b-4793-85e7-d27467a8fdf1"
      },
      "source": [
        "\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.3970,  0.4024,  1.0612,  ..., -0.0136, -0.3363,  0.6442],\n",
            "        [-0.5197,  1.0395,  0.2092,  ..., -0.8857, -0.2294,  0.1244],\n",
            "        [ 0.0057, -0.0707, -0.0804,  ..., -0.3292, -0.0130,  0.0716]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDarPNdCEMdC"
      },
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG07-m3jEQka"
      },
      "source": [
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZwGNJuREUmU"
      },
      "source": [
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVmkmPPXEYXZ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOQU7MQsEcs0"
      },
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXHdWtoREgms"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a8vIA98Emyb",
        "outputId": "39708b0f-5a0e-45bd-952a-9c964312bbe7"
      },
      "source": [
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 2m 42s\n",
            "\tTrain Loss: 0.664 | Train Acc: 59.36%\n",
            "\t Val. Loss: 0.597 |  Val. Acc: 68.39%\n",
            "Epoch: 02 | Epoch Time: 2m 44s\n",
            "\tTrain Loss: 0.606 | Train Acc: 67.44%\n",
            "\t Val. Loss: 0.483 |  Val. Acc: 77.43%\n",
            "Epoch: 03 | Epoch Time: 2m 45s\n",
            "\tTrain Loss: 0.544 | Train Acc: 72.73%\n",
            "\t Val. Loss: 0.402 |  Val. Acc: 82.49%\n",
            "Epoch: 04 | Epoch Time: 2m 44s\n",
            "\tTrain Loss: 0.401 | Train Acc: 82.48%\n",
            "\t Val. Loss: 0.343 |  Val. Acc: 84.91%\n",
            "Epoch: 05 | Epoch Time: 2m 46s\n",
            "\tTrain Loss: 0.317 | Train Acc: 87.32%\n",
            "\t Val. Loss: 0.297 |  Val. Acc: 87.68%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzs-szV5EuxY",
        "outputId": "0666f4a7-2d18-44d2-f6fa-adcc158621c8"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.360 | Test Acc: 85.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VlYA3TME0X9"
      },
      "source": [
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4KJ-w-4E4hC",
        "outputId": "013d516a-b43e-48d8-b784-4ab2de3963d7"
      },
      "source": [
        "\n",
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.023518091067671776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBnF8ISgE5id",
        "outputId": "2634e789-3849-4996-c0e7-7b86c57516e7"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9810050129890442"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDEslvBIc_IS"
      },
      "source": [
        ""
      ],
      "execution_count": 90,
      "outputs": []
    }
  ]
}